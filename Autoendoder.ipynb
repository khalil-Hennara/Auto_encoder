{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respected-albania",
   "metadata": {},
   "source": [
    "This Project is meant to be a **theoritical** Cource I provide some code but in the abstract way you can readit to understand the concept we refer to some reference in this notebook and the most important reference is \n",
    "[Hands‑On Machine Learning with Scikit‑Learn, Keras] (https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow-dp-1492032646/dp/1492032646/ref=dp_ob_image_bk) it is amazing book and I recomend every one have an interest in machince learning and Deep learning to read it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-reynolds",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "**Autoencoders** are artificial **neural networks** capable of learning dense representations\n",
    "of the input data $PDF$, called *latent representations* or codings, without any *supervision*.\n",
    "\n",
    "\n",
    "what mean by that it's **unsuprvised Learning** the **Goal** of this type of learning is to learn hidden structure of data. \n",
    "\n",
    "These codings typically have a much lower *dimensionality ==latent* than the input data, making autoencoders useful for **dimensionality reduction**\n",
    "\n",
    "Autoencoders also act as **feature detectors** and pass the output of this model to **Neural Network**.\n",
    "\n",
    "Autoencoders are also **generative models**.\n",
    "\n",
    "# Generative models:\n",
    "take as input training samples and learn a model the represent of that distribution\n",
    "\n",
    "*example*: you could train an autoencoder on pictures of faces, and it would\n",
    "then be able to generate new faces\n",
    "\n",
    "However, the generated images are usually fuzzy and not entirely realistic\n",
    "\n",
    "faces generated by **generative adversarial networks (GANs)** are now so\n",
    "convincing that it is hard to believe that the people they represent do not exist. You\n",
    "can judge so for yourself by visiting https://thispersondoesnotexist.com/, a website that\n",
    "shows faces generated by a recent GAN architecture called StyleGAN.\n",
    "\n",
    "you can read more about **GAN** hear http://GAN.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-drink",
   "metadata": {},
   "source": [
    "# How Autoencoders work?\n",
    "Autoencoders simply learn to copy their inputs to their outputs. This may sound\n",
    "like a trivial task, but we will see that constraining the network in **various ways**\n",
    "can make it rather difficult. For example, you can limit the size of the latent representations,\n",
    "or you can add **noise** to the inputs and train the **network** to recover\n",
    "the original inputs. These constraints prevent the autoencoder from trivially\n",
    "copying the inputs directly to the outputs, which forces it to learn efficient ways\n",
    "of representing the data. In short, the codings are byproducts of the autoencoder\n",
    "learning the identity function under some constraints\n",
    "\n",
    "an autoencoder looks at the inputs, converts them to an efficient **latent representation**, and then spits out some‐\n",
    "thing that (hopefully) looks very close to the inputs. An autoencoder is always composed of two parts: an **encoder (or recognition network)** that converts the inputs to a\n",
    "latent representation, followed by a **decoder (or generative network)** that converts the\n",
    "internal representation to the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-charity",
   "metadata": {},
   "source": [
    "As you can see, an autoencoder typically has the same architecture as a **Multi-Layer\n",
    "Perceptron** except that the number of **neurons** in the output\n",
    "layer must be equal to the number of inputs. In this example, there is just one hidden\n",
    "layer composed of two neurons (the **encoder**), and one output layer composed of\n",
    "three neurons (the **decoder**). The outputs are often called the **reconstructions** because\n",
    "the autoencoder tries to reconstruct the inputs, and the **cost function** contains a\n",
    "reconstruction loss that penalizes the model when the reconstructions are different\n",
    "from the inputs.\n",
    "example of reconstruction loss $ min||X_i - X_(output) ||^2$ where $X_i $ is traning example.\n",
    "\n",
    "Because the internal representation has a lower dimensionality than the input data (it\n",
    "is 2D instead of 3D), the autoencoder is said to be *undercomplete*. An undercomplete\n",
    "autoencoder cannot trivially copy its inputs to the codings, yet it must find a way to\n",
    "output a copy of its inputs. It is forced to learn the most important features in the\n",
    "input data (and drop the unimportant ones).\n",
    "<img src=\"./autoencoder_arctecture.png\" />\n",
    "Let’s see how to implement a very simple **undercomplete autoencoder** for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-minimum",
   "metadata": {},
   "source": [
    "# undercomplete autoencoder\n",
    "in order to force the autoencoder to learn interesting features, we have\n",
    "limited the size of the coding layer,and that what we called undercomplete and the kind of this autoencoders are (**basic Autoencoder**, **Stacked Autoencoder**, **CNN Autoencoder**, **RNN Autoencoder**) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-collection",
   "metadata": {},
   "source": [
    "# Performing PCA with an Undercomplete Linear Autoencoder\n",
    "If the autoencoder uses only linear activations and the cost function is the mean\n",
    "squared error **mean squre error(MSE)**, then it ends up performing **Principal Component Analysis(PCA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-latvia",
   "metadata": {},
   "source": [
    "The following code builds a simple linear autoencoder to perform **PCA** on a 3D dataset, projecting it to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code \n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "            keras.layers.Dense(2, input_shape=[3])\n",
    "         ])\n",
    "\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "            keras.layers.Dense(3, input_shape=[2])\n",
    "         ])\n",
    "\n",
    "\n",
    "autoencoder = keras.models.Sequential([\n",
    "              encoder, \n",
    "              decoder\n",
    "              ])\n",
    "\n",
    "autoencoder.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-flower",
   "metadata": {},
   "source": [
    "as we can see there is no activation function in all layer and the neuron in output layer is the same as input layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_data, X_data)   Note that X_data is the input and the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-separate",
   "metadata": {},
   "source": [
    "### Note:\n",
    "You can think of autoencoders as a form of self-supervised learning\n",
    "(i.e., using a supervised learning technique with automatically generated labels, in this case simply equal to the inputs).\n",
    "\n",
    "<img src='./pca_autoencoder.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-gross",
   "metadata": {},
   "source": [
    "# Stacked Autoencoders\n",
    "\n",
    "Just like other **neural networks** we have discussed, autoencoders can have multiple\n",
    "**hidden layers**. In this case they are called **stacked autoencoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-detroit",
   "metadata": {},
   "source": [
    "The architecture of a **stacked autoencoder** is typically symmetrical with regard to the\n",
    "central **hidden layer** (the coding layer). To put it simply, it looks like a sandwich. For\n",
    "example, an autoencoder for *MNIST*  may have 784 inputs,\n",
    "followed by a hidden layer with 100 neurons, then a central hidden layer of 30 neu‐\n",
    "rons, then another hidden layer with 100 neurons, and an output layer with 784 neurons.\n",
    "<img src='./stacked_autoencoder.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "stacked_encoder = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        keras.layers.Dense(100, activation=\"selu\"),\n",
    "        keras.layers.Dense(30, activation=\"selu\"),\n",
    "    ])\n",
    "stacked_decoder = keras.models.Sequential([\n",
    "        keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "        keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "        keras.layers.Reshape([28, 28])\n",
    "    ])\n",
    "\n",
    "stacked_ae = keras.models.Sequential([\n",
    "                stacked_encoder,\n",
    "                stacked_decoder\n",
    "             ])\n",
    "\n",
    "stacked_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.5))\n",
    "\n",
    "history = stacked_ae.fit(X_train, X_train,...) \n",
    "\n",
    "you can use this code to train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-chain",
   "metadata": {},
   "source": [
    "# Unsupervised Pretraining Using Stacked Autoencoders\n",
    "\n",
    "f you have a large dataset but most of it is unlabeled, you can first train a\n",
    "**stacked autoencoder** using all the data, then reuse the lower layers to create a **neural\n",
    "network** for your actual task and train it using the **labeled data**. For example,\n",
    "*Figure* shows how to use a stacked **autoencoder** to perform **unsupervised** **pretraining** \n",
    "for a **classification** neural network. When training the **classifier**, if you really\n",
    "don’t have much labeled training data, you may want to **freeze** the pretrained layers\n",
    "(at least the lower ones).\n",
    "\n",
    "<img src=\"./unsupervied_pretraining.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-graduation",
   "metadata": {},
   "source": [
    "# Tying Weights\n",
    "\n",
    "When an autoencoder is neatly **symmetrical** by symmetrical autoencoder we mean that the encoder layer is as the decoder layer in the inverse way, a common\n",
    "technique is to *tie* the **weights** of the **decoder** layers to the weights of the **encoder** layers.\n",
    "This halves the number of weights in the model, speeding up training and limiting the risk of **overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-respect",
   "metadata": {},
   "source": [
    "this is an aowsem technique to use,this come from linear algebra.\n",
    "\n",
    "if i **multiplie matrix** $M$ by **vector** **v** then the result is vector **u**.\n",
    "if the $M$ is invertabel then I can multiplie **u** with $M^-1$ to return back to **v**\n",
    "\n",
    "and as we know if the Matrix is  Symmetric then the inverse is $M^T$.\n",
    "\n",
    "the idea if Tying Weights it's depend on the previous explaining the code below show the coustom DenseTranspose layer. \n",
    "\n",
    "source for multiplie matrix https://www.youtube.com/watch?v=or6C4yBk_SY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "class DenseTranspose(keras.layers.Layer):\n",
    "        def __init__(self, dense, activation=None, **kwargs):\n",
    "            self.dense = dense\n",
    "            self.activation = keras.activations.get(activation)\n",
    "            super().__init__(**kwargs)\n",
    "\n",
    "        def build(self, batch_input_shape):\n",
    "            self.biases = self.add_weight(name=\"bias\", initializer=\"zeros\",shape=[self.dense.input_shape[-1]])\n",
    "            super().build(batch_input_shape)\n",
    "        def call(self, inputs):\n",
    "            z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True) \n",
    "            // this function multiplie by the transpose of the layer we pass to the constuctor\n",
    "            return self.activation(z + self.biases)\n",
    "            \n",
    "dense_1 = keras.layers.Dense(100, activation=\"selu\")\n",
    "dense_2 = keras.layers.Dense(30, activation=\"selu\")\n",
    "tied_encoder = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        dense_1,\n",
    "        dense_2\n",
    "    ])\n",
    "\n",
    "tied_decoder = keras.models.Sequential([\n",
    "        DenseTranspose(dense_2, activation=\"selu\"),\n",
    "        DenseTranspose(dense_1, activation=\"sigmoid\"),\n",
    "        keras.layers.Reshape([28, 28])\n",
    "    ])\n",
    "\n",
    "tied_ae = keras.models.Sequential([tied_encoder, tied_decoder])\n",
    "\n",
    "//then you can train this model much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-insulation",
   "metadata": {},
   "source": [
    "# Training One Autoencoder at a Time\n",
    "\n",
    "Rather than training the whole **stacked autoencoder** in one go like we just did, it is\n",
    "possible to train one **shallow autoencoder** at a time, then stack all of them into a single stacked autoencoder (hence the name), as shown in Figure This technique is not used as much these days, but you may still run into papers that talk about “greedy layerwise training,” so it’s good hear about you can search for more information.\n",
    "<img src='./Trainig_one_attime.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-theme",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoders\n",
    "\n",
    "**convolutional neural** networks are far better suited than **dense networks** to work with images. So if\n",
    "you want to build an autoencoder for images (e.g., for unsupervised pretraining or dimensionality reduction), you will need to build a **convolutional autoencoder**$^1$ https://homl.info/convae.\n",
    "\n",
    "\n",
    "$1-Jonathan Masci et al., “Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction,” Proceed‐\n",
    "ings of the 21st International Conference on Artificial Neural Networks 1 (2011): 52–59.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-northern",
   "metadata": {},
   "source": [
    "The **encoder** is a regular **CNN** composed of **convolutional layers** and **pooling layers**.\n",
    "\n",
    "while **decoder** do the reverse (upscale the image and reduce its depth back to the original dimensions), and\n",
    "for this you can use **transpose convolutional layers** (alternatively, you could combine upsampling layers with convolutional layers)\n",
    "\n",
    "the follwing code show simple *Convolutional Autoencoders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "conv_encoder = keras.models.Sequential([\n",
    "        keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),\n",
    "        keras.layers.Conv2D(16, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
    "        keras.layers.MaxPool2D(pool_size=2),\n",
    "        keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
    "        keras.layers.MaxPool2D(pool_size=2),\n",
    "        keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
    "        keras.layers.MaxPool2D(pool_size=2)\n",
    "    ])\n",
    "\n",
    "conv_decoder = keras.models.Sequential([\n",
    "        keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"valid\",activation=\"selu\",\n",
    "        input_shape=[3, 3, 64]),\n",
    "        keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"same\",activation=\"selu\"),\n",
    "        keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"same\",activation=\"sigmoid\"),\n",
    "        keras.layers.Reshape([28, 28])\n",
    "    ])\n",
    "    \n",
    "conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-regulation",
   "metadata": {},
   "source": [
    "# Recurrent Autoencoders\n",
    "\n",
    "If you want to build an autoencoder for **sequences**, such as **time series** or text (e.g., for unsupervised learning or dimensionality reduction), then **recurrent neural networks**\n",
    "may be better suited than **dense networks**. Building a recurrent\n",
    "autoencoder is straightforward: the **encoder** is typically a **sequence-to-vector RNN**\n",
    "which compresses the input sequence down to a single vector. The **decoder** is a\n",
    "**vector-to-sequence RNN** that does the reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recurrent_encoder = keras.models.Sequential([\n",
    "            keras.layers.LSTM(100, return_sequences=True, input_shape=[None, 28]),\n",
    "            keras.layers.LSTM(30)\n",
    "        ])\n",
    "\n",
    "recurrent_decoder = keras.models.Sequential([\n",
    "            keras.layers.RepeatVector(28, input_shape=[30]),\n",
    "            keras.layers.LSTM(100, return_sequences=True),\n",
    "            keras.layers.TimeDistributed(keras.layers.Dense(28, activation=\"sigmoid\"))\n",
    "        ])\n",
    "\n",
    "recurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])\n",
    "\n",
    "//Note that we use a RepeatVector layer as the first layer of the decoder, to ensure that its input vector gets fed to the decoder at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-peter",
   "metadata": {},
   "source": [
    "# overcomplete autoencoder\n",
    "There are actually many other kinds of constraints that can be used, including ones that allow the coding layer to be just as large as the inputs, or even larger and the kind of this autoencoders are (**Denoising Autoencoders**, **Sparse Autoencoders**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-evidence",
   "metadata": {},
   "source": [
    "# Denoising Autoencoders\n",
    "\n",
    "\n",
    "Another way to force the autoencoder to learn useful features is to add **noise** to its\n",
    "inputs, training it to recover the original, noise-free inputs. This idea has been around\n",
    "since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a 2008\n",
    "$paper^1$ Pascal Vincent et al. showed that autoencoders could also be used for feature\n",
    "extraction. In a 2010 $paper^2$ Vincent et al. introduced **stacked denoising autoencoders**.\n",
    "\n",
    "The noise can be pure **Gaussian** noise added to the inputs, or it can be randomly\n",
    "switched-off inputs, just like in **dropout**. Figure shows both options.\n",
    "\n",
    "$1-Pascal Vincent et al., “Extracting and Composing Robust Features with Denoising Autoencoders,” Proceedings\n",
    "of the 25th International Conference on Machine Learning (2008): 1096–1103.$\n",
    "\n",
    "$2-Pascal Vincent et al.“Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network\n",
    "with a Local Denoising Criterion,” Journal of Machine Learning Research 11 (2010): 3371–3408.$\n",
    "\n",
    "<img src ='./denoising_auto.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code \n",
    "import tensorflow.keras as keras\n",
    "\n",
    "// the Dropout noise \n",
    "dropout_encoder = keras.models.Sequential([\n",
    "            keras.layers.Flatten(input_shape=[28, 28]),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(100, activation=\"selu\"),\n",
    "            keras.layers.Dense(30, activation=\"selu\")\n",
    "        ])\n",
    "dropout_decoder = keras.models.Sequential([\n",
    "            keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "            keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "            keras.layers.Reshape([28, 28])\n",
    "        ])\n",
    "        \n",
    "dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])\n",
    "\n",
    "// the Gaussian noise \n",
    "dropout_encoder = keras.models.Sequential([\n",
    "            keras.layers.Flatten(input_shape=[28, 28]),\n",
    "            keras.layers.GaussianNoise(),\n",
    "            keras.layers.Dense(100, activation=\"selu\"),\n",
    "            keras.layers.Dense(30, activation=\"selu\")\n",
    "        ])\n",
    "dropout_decoder = keras.models.Sequential([\n",
    "            keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "            keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "            keras.layers.Reshape([28, 28])\n",
    "        ])\n",
    "        \n",
    "dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])\n",
    "\n",
    "//Gaussian and dropout layer thay both active only during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-parliament",
   "metadata": {},
   "source": [
    "# Sparse Autoencoders\n",
    "\n",
    "Another kind of constraint that often leads to good feature extraction is **sparsity**: by\n",
    "adding an appropriate term to the cost function, the autoencoder is pushed to reduce\n",
    "the number of active neurons in the coding layer. For example, it may be pushed to\n",
    "have on average only 5% significantly active neurons in the coding layer. This forces\n",
    "the autoencoder to represent each input as a combination of a small number of activations. As a result, each neuron in the coding layer typically ends up representing a\n",
    "useful feature\n",
    "\n",
    "A simple approach is to use the **sigmoid** activation function in the **coding layer** (to\n",
    "constrain the codings to values between 0 and 1), use a large coding layer and add some $ℓ_1$ **regularization** to the coding layer’s activations.\n",
    "\n",
    "agood source about $l_1$ norm and way is good to present sparsity vector. https://www.youtube.com/watch?v=NcPUI7aPFhA&t=7s "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-estonia",
   "metadata": {},
   "source": [
    "we want to **penalize** the neurons that are too active, or not active enough, by adding a sparsity loss to the **cost function**.\n",
    "For example, if we measure that a neuron has an average activation of 0.3, but the target\n",
    "sparsity is 0.1, it must be penalized to activate less. One approach could be simply\n",
    "adding the squared error $(0.3 – 0.1)^2$ to the cost function, but in practice a better\n",
    "approach is to use the Kullback–Leibler **(KL) divergence**  which has much stronger gradients than the mean squared error\n",
    "\n",
    "Given two discrete **probability distributions** P and Q, the **KL divergence** between\n",
    "these distributions, noted D KL (P ∥ Q), can be computed\n",
    "\n",
    "$KL(P,Q)=∑_i P_i log \\frac {(P_i)}{(Q_i)}$\n",
    "\n",
    "which is the **cross-Entropy** - **Entropy**\n",
    "\n",
    "cross-Entopy :$H(P,Q)=∑_i -P_i log(Q_i) $ and that is the loss function called **categorial-crossentropy**\n",
    "\n",
    "Entropy : $ E(P) =∑_i -P_i log(P_i)$ is avery important topic in **information theory** \n",
    "\n",
    "In our case, we want to measure the divergence between the target probability $p$ that a\n",
    "neuron in the coding layer will activate and the actual probability $q$ (i.e., the mean\n",
    "activation over the training batch). So the **KL divergence** simplifies\n",
    "\n",
    "$D(p,q) = p  log \\frac {p}{ q }+ (1 − p) log \\frac{(1-p)}{(1-q)}$\n",
    "\n",
    "Once we have computed the sparsity loss for each neuron in the **coding layer**, we sum\n",
    "up these losses and add the result to the **cost function**. In order to control the relative\n",
    "importance of the sparsity loss and the reconstruction loss, we can multiply the spar‐\n",
    "sity loss by a sparsity weight **hyperparameter**. If this weight is too high, the model will\n",
    "stick closely to the target sparsity, but it may not reconstruct the inputs properly,\n",
    "making the model useless. Conversely, if it is too low, the model will mostly ignore\n",
    "the sparsity objective and will not learn any interesting features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-sphere",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "Another important category of autoencoders was introduced in 2013 by Diederik\n",
    "Kingma and Max Welling and quickly became one of the most popular types of\n",
    "autoencoders called **variational autoencoders**$^1.$\n",
    "\n",
    "They are quite different from all the autoencoders we have discussed so far, in these\n",
    "particular ways:\n",
    "\n",
    "    • They are probabilistic autoencoders, meaning that their outputs are partly determined by chance, even after training (as opposed to denoising autoencoders,which use randomness only during training).\n",
    "\n",
    "    • Most importantly, they are generative autoencoders, meaning that they can generate new instances that look like they were sampled from the training set.\n",
    "\n",
    "$1-Diederik Kingma and Max Welling, “Auto-Encoding Variational Bayes,” arXiv preprint arXiv:1312.6114\n",
    "(2013).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-invention",
   "metadata": {},
   "source": [
    "# How it's work?\n",
    "\n",
    "Let’s take a look at how they work. Figure (left) shows a variational autoencoder. You can recognize the basic structure of all autoencoders, with an encoder followed by a decoder (in this example, they both have two hidden layers), but there is a twist: instead of directly producing a coding for a given input, the encoder produces a\n",
    "**mean coding μ** and a **standard deviation σ**. The actual coding is then sampled randomly from a **Gaussian distribution** with mean μ and standard deviation σ. After that\n",
    "the decoder decodes the sampled coding normally. The right part of the diagram\n",
    "shows a training instance going through this autoencoder. First, the encoder pro‐\n",
    "duces μ and σ, then a coding is sampled randomly (notice that it is not exactly located\n",
    "at μ), and finally this coding is decoded; the final output resembles the training\n",
    "instance.\n",
    "\n",
    "<img src='./Variational_autoencoder_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-briefs",
   "metadata": {},
   "source": [
    "As you can see in the diagram, although the inputs may have a very convoluted distribution, a variational autoencoder tends to produce codings that look as though they were sampled from a simple **Gaussian distribution** during training, the **cost function** pushes the codings to gradually migrate within the coding space (also called the latent space) to end up looking like a cloud of Gaussian points.\n",
    "\n",
    "One great consequence is that after training a variational autoencoder, you can very easily\n",
    "generate a new instance: just sample a random coding from the Gaussian distribution, decode it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-combination",
   "metadata": {},
   "source": [
    "Now, let’s look at the cost function. It is composed of two parts. The first is the usual\n",
    "reconstruction loss that pushes the autoencoder to reproduce its inputs (we can use\n",
    "cross entropy for this, as discussed earlier). The second is the latent loss that pushes\n",
    "the autoencoder to have codings that look as though they were sampled from a simple\n",
    "Gaussian distribution: it is the KL divergence between the target distribution (i.e., the\n",
    "Gaussian distribution) and the actual distribution of the codings. The math is a bit\n",
    "more complex than with the sparse autoencoder, in particular because of the Gaus‐\n",
    "sian noise, which limits the amount of information that can be transmitted to the\n",
    "coding layer (thus pushing the autoencoder to learn useful features). Luckily, the\n",
    "\n",
    "equations simplify, so the **latent loss** can be computed quite simply using\n",
    "\n",
    "$ℒ = −\\frac{1}{2} \\sum_{i=0}^{K} 1+log (σ_i ^2) − σ_i^ 2 − μ_i^ 2 $\n",
    "\n",
    "In this equation, ℒ is the **latent loss**, n is the codings’ dimensionality, and $μ_i$ and $σ_i$ are\n",
    "the **mean** and **standard deviation** of the i th component of the codings. The vectors **μ**\n",
    "and **σ** (which contain all the $μ_i$ and $σ_i$ ) are output by the encoder\n",
    "\n",
    "A common tweak to the variational autoencoder’s architecture is to make the encoder\n",
    "output $γ = log(σ^2 )$ rather than σ. The **latent loss** can then be computed as shown in following\n",
    "Equation. \n",
    "\n",
    "$ℒ = -\\frac{1}{2}\\sum_{i=0}^{K}1+γ_i − exp (γ_i) − μ_i^2$\n",
    "\n",
    "This approach is more numerically stable and speeds up training.\n",
    "\n",
    "good source for learn more about *Variational Autoencoders*:\n",
    "\n",
    "*1-Variational autoencoders by JERMY JORDON*: https://www.jeremyjordan.me/variational-autoencoders/\n",
    "\n",
    "*2-Understanding Variational Autoencoders (VAEs) by jOSEPH ROCCA:* https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n",
    "\n",
    "*3-Tutorial - What is a variational autoencoder? by JANN ALTOSAAR*:https://jaan.io/what-is-variational-autoencoder-vae-tutorial/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is for MINST data set \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class Sampling(keras.layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean\n",
    "\n",
    "\n",
    "codings_size = 10\n",
    "inputs = keras.layers.Input(shape=[28, 28])\n",
    "z = keras.layers.Flatten()(inputs)\n",
    "z = keras.layers.Dense(150, activation=\"selu\")(z)\n",
    "z = keras.layers.Dense(100, activation=\"selu\")(z)\n",
    "codings_mean = keras.layers.Dense(codings_size)(z) # μ\n",
    "codings_log_var = keras.layers.Dense(codings_size)(z) # γ\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "variational_encoder = keras.Model(inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
    "x = keras.layers.Dense(100, activation=\"selu\")(decoder_inputs)\n",
    "x = keras.layers.Dense(150, activation=\"selu\")(x)\n",
    "x = keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n",
    "outputs = keras.layers.Reshape([28, 28])(x)\n",
    "variational_decoder = keras.Model(inputs=[decoder_inputs], outputs=[outputs])\n",
    "\n",
    "_, _, codings = variational_encoder(inputs)\n",
    "reconstructions = variational_decoder(codings)\n",
    "variational_ae = keras.Model(inputs=[inputs], outputs=[reconstructions])\n",
    "\n",
    "\n",
    "latent_loss = -0.5 * K.sum(1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),axis=-1)\n",
    "\n",
    "variational_ae.add_loss(K.mean(latent_loss) / 784.)\n",
    "\n",
    "variational_ae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-celtic",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
